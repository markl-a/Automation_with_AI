## å·¥ä½œæµè‡ªå‹•åŒ–é›†æˆæŒ‡å—
# Workflow Automation Integration Guide

æœ¬æŒ‡å—è©³ç´°ä»‹ç´¹å¦‚ä½•ä½¿ç”¨ AI Automation Framework é›†æˆå„ç¨®å·¥ä½œæµè‡ªå‹•åŒ–å¹³å°ã€‚

---

## ğŸ“‹ ç›®éŒ„

- [æ”¯æŒçš„å¹³å°](#æ”¯æŒçš„å¹³å°)
- [å¿«é€Ÿé–‹å§‹](#å¿«é€Ÿé–‹å§‹)
- [å¹³å°é›†æˆè©³è§£](#å¹³å°é›†æˆè©³è§£)
  - [n8n](#n8n-é›†æˆ)
  - [Make (Integromat)](#make-integromat-é›†æˆ)
  - [Zapier](#zapier-é›†æˆ)
  - [Apache Airflow](#apache-airflow-é›†æˆ)
  - [Temporal](#temporal-é›†æˆ)
  - [Prefect](#prefect-é›†æˆ)
  - [Celery](#celery-é›†æˆ)
- [çµ±ä¸€æ¥å£ä½¿ç”¨](#çµ±ä¸€æ¥å£ä½¿ç”¨)
- [é«˜ç´šç”¨æ³•](#é«˜ç´šç”¨æ³•)
- [æœ€ä½³å¯¦è¸](#æœ€ä½³å¯¦è¸)
- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

---

## æ”¯æŒçš„å¹³å°

| å¹³å° | é¡å‹ | é–‹æº | è‡ªè¨—ç®¡ | æ”¯æŒåŠŸèƒ½ |
|------|------|------|--------|---------|
| **n8n** | å·¥ä½œæµè‡ªå‹•åŒ– | âœ… | âœ… | Webhookã€APIã€å·¥ä½œæµç®¡ç† |
| **Make** | å·¥ä½œæµè‡ªå‹•åŒ– | âŒ | âŒ | Webhookã€å ´æ™¯ç®¡ç† |
| **Zapier** | å·¥ä½œæµè‡ªå‹•åŒ– | âŒ | âŒ | Webhookã€Zap è§¸ç™¼ |
| **Apache Airflow** | æ•¸æ“šç®¡é“ | âœ… | âœ… | DAG ç®¡ç†ã€åŸ·è¡Œç›£æ§ |
| **Temporal** | åˆ†å¸ƒå¼å·¥ä½œæµå¼•æ“ | âœ… | âœ… | å·¥ä½œæµç·¨æ’ã€é•·æ™‚é–“é‹è¡Œä»»å‹™ã€ç‹€æ…‹ç®¡ç† |
| **Prefect** | ç¾ä»£æ•¸æ“šå·¥ä½œæµ | âœ… | âœ… | Flow ç·¨æ’ã€èª¿åº¦ã€ç›£æ§ |
| **Celery** | åˆ†å¸ƒå¼ä»»å‹™éšŠåˆ— | âœ… | âœ… | ç•°æ­¥ä»»å‹™ã€å®šæ™‚ä»»å‹™ã€ä»»å‹™éˆ |

---

## å¿«é€Ÿé–‹å§‹

### 1. å®‰è£ä¾è³´

```bash
pip install -r requirements.txt
```

### 2. è¨­ç½®ç’°å¢ƒè®Šé‡

å‰µå»º `.env` æ–‡ä»¶ï¼š

```bash
# n8n
N8N_BASE_URL=http://localhost:5678
N8N_API_KEY=your_n8n_api_key

# Make
MAKE_API_TOKEN=your_make_api_token
MAKE_ORGANIZATION_ID=your_org_id
MAKE_TEAM_ID=your_team_id

# Zapier
ZAPIER_WEBHOOK_URL=https://hooks.zapier.com/hooks/catch/xxx/yyy/
ZAPIER_API_KEY=your_zapier_api_key

# Airflow
AIRFLOW_BASE_URL=http://localhost:8080
AIRFLOW_USERNAME=admin
AIRFLOW_PASSWORD=admin

# Temporal
TEMPORAL_HOST=localhost:7233
TEMPORAL_NAMESPACE=default
TEMPORAL_TASK_QUEUE=ai-automation

# Prefect
# Prefect uses local configuration by default

# Celery
CELERY_BROKER_URL=redis://localhost:6379/0
CELERY_RESULT_BACKEND=redis://localhost:6379/0
```

### 3. åŸºæœ¬ä½¿ç”¨

```python
from ai_automation_framework.integrations.workflow_automation_unified import (
    UnifiedWorkflowManager,
    WorkflowPlatform
)

# å‰µå»ºç®¡ç†å™¨
manager = UnifiedWorkflowManager()

# è¨»å†Š n8n
manager.register_n8n(
    base_url="http://localhost:5678",
    api_key="your_api_key"
)

# è§¸ç™¼å·¥ä½œæµ
result = manager.trigger_workflow(
    platform=WorkflowPlatform.N8N,
    workflow_id="workflow_123",
    data={"key": "value"}
)

print(result)
```

---

## å¹³å°é›†æˆè©³è§£

### n8n é›†æˆ

#### åŸºæœ¬åŠŸèƒ½

```python
from ai_automation_framework.integrations.n8n_integration_enhanced import N8NEnhanced

# åˆå§‹åŒ–
n8n = N8NEnhanced(
    base_url="http://localhost:5678",
    api_key="your_api_key"
)

# 1. å·¥ä½œæµç®¡ç†
workflows = n8n.get_workflows(active=True)
workflow = n8n.get_workflow("workflow_id")

# 2. åŸ·è¡Œå·¥ä½œæµ
result = n8n.execute_workflow(
    workflow_id="workflow_123",
    data={"input": "data"}
)

# 3. ç›£æ§åŸ·è¡Œ
execution = n8n.get_execution(result['data']['id'])
print(f"ç‹€æ…‹: {execution['data']['status']}")

# 4. Webhook è§¸ç™¼
webhook_result = n8n.trigger_webhook(
    webhook_id="webhook_path",
    data={"event": "new_order"}
)
```

#### å‰µå»º AI å·¥ä½œæµæ¨¡æ¿

```python
# å‰µå»º AI è™•ç†å·¥ä½œæµ
template = n8n.create_ai_workflow_template(
    name="AI Content Generator",
    webhook_path="ai-content",
    ai_prompt="You are a helpful content writer..."
)

# å‰µå»ºå·¥ä½œæµ
result = n8n.create_workflow(
    name=template['name'],
    nodes=template['nodes'],
    connections=template['connections'],
    active=True
)
```

#### æ‰¹é‡åŸ·è¡Œ

```python
# æ‰¹é‡åŸ·è¡Œå·¥ä½œæµ
data_list = [
    {"customer": "John", "order": 1},
    {"customer": "Jane", "order": 2},
    {"customer": "Bob", "order": 3}
]

results = n8n.bulk_execute("workflow_id", data_list)

for result in results:
    print(f"åŸ·è¡Œ: {result.get('success')}")
```

#### ç­‰å¾…åŸ·è¡Œå®Œæˆ

```python
# è§¸ç™¼å·¥ä½œæµ
execution = n8n.execute_workflow("workflow_id", {"data": "test"})
execution_id = execution['data']['id']

# ç­‰å¾…å®Œæˆï¼ˆæœ€å¤š 5 åˆ†é˜ï¼‰
final_result = n8n.wait_for_execution(
    execution_id=execution_id,
    max_wait=300,
    poll_interval=2
)

if final_result.get('success'):
    print(f"åŸ·è¡Œç‹€æ…‹: {final_result['data']['status']}")
```

---

### Make (Integromat) é›†æˆ

#### åŸºæœ¬åŠŸèƒ½

```python
from ai_automation_framework.integrations.make_integration import MakeIntegration

# åˆå§‹åŒ–
make = MakeIntegration(
    api_token="your_token",
    organization_id="org_id",
    team_id="team_id"
)

# 1. å ´æ™¯ç®¡ç†
scenarios = make.get_scenarios()
scenario = make.get_scenario("scenario_id")

# 2. åŸ·è¡Œå ´æ™¯
result = make.run_scenario(
    scenario_id="scenario_123",
    data={"input": "data"}
)

# 3. æ¿€æ´»/åœç”¨å ´æ™¯
make.activate_scenario("scenario_id")
make.deactivate_scenario("scenario_id")
```

#### Webhook è§¸ç™¼

```python
# æ–¹å¼ 1: ç›´æ¥ URL
result = make.trigger_webhook(
    webhook_url="https://hook.eu1.make.com/xxx",
    data={"event": "order_created"}
)

# æ–¹å¼ 2: ä½¿ç”¨ webhook key
result = make.trigger_custom_webhook(
    webhook_key="your_webhook_key",
    data={"event": "order_created"}
)
```

#### æ•¸æ“šå­˜å„²

```python
# ç²å–æ•¸æ“šå­˜å„²
datastores = make.get_data_stores()

# è®€å–è¨˜éŒ„
records = make.get_data_store_records(
    datastore_id="ds_123",
    limit=100
)

# æ·»åŠ è¨˜éŒ„
result = make.add_data_store_record(
    datastore_id="ds_123",
    data={"name": "John", "email": "john@example.com"}
)
```

---

### Zapier é›†æˆ

#### åŸºæœ¬åŠŸèƒ½

```python
from ai_automation_framework.integrations.zapier_integration_enhanced import ZapierEnhanced

# åˆå§‹åŒ–
zapier = ZapierEnhanced(
    default_webhook_url="https://hooks.zapier.com/hooks/catch/xxx/yyy/",
    api_key="your_api_key"
)

# 1. è§¸ç™¼ Webhook
result = zapier.trigger_webhook({
    "event": "new_user",
    "name": "John Doe",
    "email": "john@example.com"
})

# 2. æ‰¹é‡è§¸ç™¼
results = zapier.batch_trigger(
    data_list=[
        {"user": "John"},
        {"user": "Jane"},
        {"user": "Bob"}
    ],
    delay_between=0.5  # 500ms å»¶é²
)
```

#### é å®šç¾©å‹•ä½œ

```python
# ç™¼é€éƒµä»¶
zapier.send_email(
    to="recipient@example.com",
    subject="Test Email",
    body="This is a test email from AI Framework",
    cc=["cc@example.com"],
    attachments=["https://example.com/file.pdf"]
)

# ç™¼é€ Slack æ¶ˆæ¯
zapier.send_slack_message(
    channel="#general",
    message="New order received!",
    username="Order Bot",
    icon_emoji=":package:"
)

# å‰µå»º Google Sheets è¡Œ
zapier.create_google_sheet_row(
    spreadsheet_id="sheet_123",
    row_data={
        "Name": "John Doe",
        "Email": "john@example.com",
        "Date": "2025-01-XX"
    }
)

# å‰µå»ºä»»å‹™
zapier.create_task(
    task_name="Follow up with customer",
    description="Contact John about the order",
    due_date="2025-02-01",
    priority="high",
    assignee="sales@example.com"
)

# ç™¼é€çŸ­ä¿¡
zapier.send_sms(
    to="+1234567890",
    message="Your order has been shipped!"
)
```

---

### Apache Airflow é›†æˆ

#### åŸºæœ¬åŠŸèƒ½

```python
from ai_automation_framework.integrations.airflow_integration import AirflowIntegration

# åˆå§‹åŒ–
airflow = AirflowIntegration(
    base_url="http://localhost:8080",
    username="admin",
    password="admin"
)

# 1. DAG ç®¡ç†
dags = airflow.list_dags()
dag = airflow.get_dag("dag_id")

# 2. è§¸ç™¼ DAG
result = airflow.trigger_dag(
    dag_id="example_dag",
    conf={"param1": "value1"}
)

# 3. æŸ¥è©¢åŸ·è¡Œç‹€æ…‹
dag_run = airflow.get_dag_run("dag_id", "run_id")
print(f"ç‹€æ…‹: {dag_run['state']}")

# 4. æš«åœ/æ¢å¾© DAG
airflow.pause_dag("dag_id")
airflow.unpause_dag("dag_id")
```

---

### Temporal é›†æˆ

Temporal æ˜¯ä¸€å€‹é–‹æºçš„åˆ†å¸ƒå¼å·¥ä½œæµå¼•æ“ï¼Œç”¨æ–¼æ§‹å»ºå¯é çš„ã€å¯æ“´å±•çš„æ‡‰ç”¨ç¨‹åºã€‚

#### åŸºæœ¬åŠŸèƒ½

```python
from ai_automation_framework.integrations.temporal_integration import TemporalIntegration
import asyncio

async def main():
    # åˆå§‹åŒ–
    temporal = TemporalIntegration(
        host="localhost:7233",
        namespace="default",
        task_queue="ai-automation"
    )

    # é€£æ¥åˆ° Temporal
    await temporal.connect()

    # 1. å®šç¾© Activity
    @temporal.create_activity()
    async def process_order(order_id: str):
        print(f"è™•ç†è¨‚å–®: {order_id}")
        return {"order_id": order_id, "status": "processed"}

    # 2. å®šç¾© Workflow
    @temporal.create_workflow()
    async def order_workflow(order_id: str):
        result = await process_order(order_id)
        return result

    # 3. å•Ÿå‹• Workflow
    result = await temporal.start_workflow(
        workflow_id="order-001",
        workflow_type="order_workflow",
        args=["ORD-12345"]
    )
    print(f"å·¥ä½œæµå·²å•Ÿå‹•: {result}")

    # 4. æŸ¥è©¢ Workflow ç‹€æ…‹
    if result.get('success'):
        status = await temporal.get_workflow_result(result['workflow_id'])
        print(f"å·¥ä½œæµç‹€æ…‹: {status}")

    # 5. ç™¼é€ä¿¡è™Ÿåˆ° Workflow
    await temporal.signal_workflow(
        workflow_id="order-001",
        signal_name="approve",
        args=[{"approved": True}]
    )

    # 6. å–æ¶ˆ Workflow
    await temporal.cancel_workflow("order-001")

asyncio.run(main())
```

#### ä½¿ç”¨å·¥ä½œæµæ§‹å»ºå™¨

```python
from ai_automation_framework.integrations.temporal_integration import TemporalWorkflowBuilder

builder = TemporalWorkflowBuilder(temporal.client)

# è¨»å†Š Activity
@builder.register_activity(name="send_notification")
async def send_notification(user_id: str, message: str):
    print(f"ç™¼é€é€šçŸ¥çµ¦ {user_id}: {message}")
    return True

# è¨»å†Š Workflow
@builder.register_workflow(name="notification_workflow")
async def notification_workflow(user_id: str):
    await send_notification(user_id, "æ‚¨æœ‰æ–°æ¶ˆæ¯")
    return {"success": True}
```

#### å®‰è£å’Œé‹è¡Œ

```bash
# å®‰è£ Temporal
pip install temporalio

# å•Ÿå‹• Temporal æœå‹™å™¨ï¼ˆé–‹ç™¼ç’°å¢ƒï¼‰
temporal server start-dev

# å•Ÿå‹• Workerï¼ˆåœ¨å¦ä¸€å€‹çµ‚ç«¯ï¼‰
python your_worker.py
```

---

### Prefect é›†æˆ

Prefect æ˜¯ä¸€å€‹ç¾ä»£åŒ–çš„æ•¸æ“šå·¥ä½œæµç·¨æ’å¹³å°ï¼Œå°ˆæ³¨æ–¼å¯è§€å¯Ÿæ€§å’Œæ˜“ç”¨æ€§ã€‚

#### åŸºæœ¬åŠŸèƒ½

```python
from ai_automation_framework.integrations.prefect_integration import PrefectIntegration
import asyncio

async def main():
    # åˆå§‹åŒ–
    prefect = PrefectIntegration()

    # 1. å®šç¾© Task
    @prefect.create_task(name="extract_data")
    async def extract_data(source: str):
        print(f"å¾ {source} æå–æ•¸æ“š")
        return {"records": 1000}

    @prefect.create_task(name="transform_data")
    async def transform_data(data: dict):
        print(f"è½‰æ› {data['records']} æ¢è¨˜éŒ„")
        return {"transformed": True}

    # 2. å®šç¾© Flow
    @prefect.create_flow(name="etl_pipeline")
    async def etl_pipeline(source: str):
        raw_data = await extract_data(source)
        result = await transform_data(raw_data)
        return result

    # 3. å‰µå»º Flow Run
    flow_run = await prefect.create_flow_run(
        flow_name="etl_pipeline",
        parameters={"source": "database"}
    )
    print(f"Flow Run å·²å‰µå»º: {flow_run}")

    # 4. ç­‰å¾… Flow å®Œæˆ
    if flow_run.get('success'):
        result = await prefect.wait_for_flow_run(flow_run['flow_run_id'])
        print(f"Flow åŸ·è¡Œçµæœ: {result}")

    # 5. ç²å– Flow Run ç‹€æ…‹
    status = await prefect.get_flow_run_status(flow_run['flow_run_id'])
    print(f"ç‹€æ…‹: {status}")

    # 6. å–æ¶ˆ Flow Run
    await prefect.cancel_flow_run(flow_run['flow_run_id'])

asyncio.run(main())
```

#### å®šæ™‚èª¿åº¦

```python
from ai_automation_framework.integrations.prefect_integration import PrefectScheduler
from datetime import timedelta

async def setup_schedules():
    scheduler = PrefectScheduler()

    # 1. Cron èª¿åº¦ï¼ˆæ¯å¤©æ—©ä¸Š 8 é»ï¼‰
    await scheduler.create_cron_schedule(
        flow_name="daily_report",
        cron="0 8 * * *",
        schedule_name="morning_report"
    )

    # 2. é–“éš”èª¿åº¦ï¼ˆæ¯å°æ™‚ï¼‰
    await scheduler.create_interval_schedule(
        flow_name="health_check",
        interval_seconds=3600,
        schedule_name="hourly_health_check"
    )

    # 3. åˆ—å‡ºæ‰€æœ‰èª¿åº¦
    schedules = await scheduler.list_schedules()
    print(f"ç•¶å‰èª¿åº¦: {schedules}")

asyncio.run(setup_schedules())
```

#### å®‰è£å’Œé‹è¡Œ

```bash
# å®‰è£ Prefect
pip install prefect

# å•Ÿå‹• Prefect æœå‹™å™¨
prefect server start

# è¨ªå• UI
open http://localhost:4200
```

---

### Celery é›†æˆ

Celery æ˜¯ä¸€å€‹åˆ†å¸ƒå¼ä»»å‹™éšŠåˆ—ï¼Œç”¨æ–¼è™•ç†å¤§é‡ç•°æ­¥ä»»å‹™ã€‚

#### åŸºæœ¬åŠŸèƒ½

```python
from ai_automation_framework.integrations.celery_integration import CeleryIntegration

# åˆå§‹åŒ–
celery = CeleryIntegration(
    broker_url="redis://localhost:6379/0",
    backend_url="redis://localhost:6379/0"
)

# 1. å®šç¾©ä»»å‹™
@celery.create_task(name="send_email")
def send_email(to: str, subject: str, body: str):
    print(f"ç™¼é€éƒµä»¶åˆ° {to}")
    return {"sent": True}

@celery.create_task(name="process_image")
def process_image(image_path: str):
    print(f"è™•ç†åœ–ç‰‡: {image_path}")
    return {"processed": True}

# 2. ç™¼é€ä»»å‹™
result = celery.send_task(
    task_name="send_email",
    kwargs={
        "to": "user@example.com",
        "subject": "Hello",
        "body": "Test message"
    }
)
print(f"ä»»å‹™å·²ç™¼é€: {result}")

# 3. ç²å–ä»»å‹™çµæœ
if result.get('success'):
    task_result = celery.get_task_result(result['task_id'])
    print(f"ä»»å‹™çµæœ: {task_result}")

# 4. å»¶é²ä»»å‹™ï¼ˆ5ç§’å¾ŒåŸ·è¡Œï¼‰
delayed_result = celery.send_task(
    task_name="send_email",
    args=["admin@example.com", "Reminder", "Don't forget!"],
    countdown=5
)

# 5. æ’¤éŠ·ä»»å‹™
celery.revoke_task(task_id=result['task_id'], terminate=True)
```

#### ä»»å‹™éˆå’Œçµ„

```python
from celery import chain, group

# 1. ä»»å‹™éˆï¼ˆé †åºåŸ·è¡Œï¼‰
@celery.create_task(name="step1")
def step1(data):
    return {"step": 1, "data": data}

@celery.create_task(name="step2")
def step2(result):
    return {"step": 2, "previous": result}

# å‰µå»ºä»»å‹™éˆ
task_chain = chain(
    step1.s("initial_data"),
    step2.s()
)

# 2. ä»»å‹™çµ„ï¼ˆä¸¦è¡ŒåŸ·è¡Œï¼‰
@celery.create_task(name="process_file")
def process_file(file_path):
    return {"file": file_path, "processed": True}

# å‰µå»ºä»»å‹™çµ„
task_group = group(
    process_file.s("/data/file1.txt"),
    process_file.s("/data/file2.txt"),
    process_file.s("/data/file3.txt")
)
```

#### é€±æœŸæ€§ä»»å‹™

```python
from datetime import timedelta
from celery.schedules import crontab

# æ·»åŠ æ¯å°æ™‚åŸ·è¡Œçš„ä»»å‹™
celery.add_periodic_task(
    schedule=timedelta(hours=1),
    task_name="cleanup_task",
    name="hourly_cleanup"
)

# æ·»åŠ æ¯å¤©å‡Œæ™¨ 2 é»çš„å‚™ä»½ä»»å‹™
celery.add_periodic_task(
    schedule=crontab(hour=2, minute=0),
    task_name="backup_task",
    name="daily_backup"
)
```

#### ä»»å‹™ç›£æ§

```python
from ai_automation_framework.integrations.celery_integration import CeleryMonitor

monitor = CeleryMonitor(celery.app)

# 1. ç²å–æ´»å‹•ä»»å‹™
active = celery.get_active_tasks()
print(f"æ´»å‹•ä»»å‹™: {active}")

# 2. ç²å–çµ±è¨ˆä¿¡æ¯
stats = monitor.get_stats()
print(f"çµ±è¨ˆä¿¡æ¯: {stats}")

# 3. Ping Workers
ping = monitor.ping_workers()
print(f"Worker ç‹€æ…‹: {ping}")
```

#### å®‰è£å’Œé‹è¡Œ

```bash
# å®‰è£ Celery å’Œ Redis
pip install celery[redis]

# å•Ÿå‹• Redis
redis-server

# å•Ÿå‹• Celery Worker
celery -A your_app worker --loglevel=info

# å•Ÿå‹• Celery Beatï¼ˆå®šæ™‚ä»»å‹™ï¼‰
celery -A your_app beat

# ç›£æ§ä»»å‹™ï¼ˆä½¿ç”¨ Flowerï¼‰
pip install flower
celery -A your_app flower
```

---

## çµ±ä¸€æ¥å£ä½¿ç”¨

### å–®å¹³å°ä½¿ç”¨

```python
from ai_automation_framework.integrations.workflow_automation_unified import (
    UnifiedWorkflowManager,
    WorkflowPlatform
)

manager = UnifiedWorkflowManager()

# è¨»å†Šå¹³å°
manager.register_n8n("http://localhost:5678", "api_key")

# è§¸ç™¼å·¥ä½œæµ
result = manager.trigger_workflow(
    platform=WorkflowPlatform.N8N,
    workflow_id="workflow_123",
    data={"key": "value"}
)
```

### å¤šå¹³å°é›†æˆ

```python
manager = UnifiedWorkflowManager()

# è¨»å†Šå¤šå€‹å¹³å°
manager.register_n8n("http://localhost:5678", "n8n_key")
manager.register_zapier(webhook_url="https://hooks.zapier.com/...")
manager.register_make(api_token="make_token")
manager.register_airflow("http://localhost:8080", "admin", "password")
manager.register_temporal(host="localhost:7233")
manager.register_prefect()
manager.register_celery(broker_url="redis://localhost:6379/0")

# å»£æ’­è§¸ç™¼
results = manager.broadcast_trigger(
    platforms=[
        WorkflowPlatform.N8N,
        WorkflowPlatform.ZAPIER,
        WorkflowPlatform.MAKE
    ],
    workflow_configs={
        "n8n": "notification_workflow",
        "zapier": "https://hooks.zapier.com/...",
        "make": "scenario_notify"
    },
    data={"event": "order_completed", "order_id": "ORD-123"}
)

# æª¢æŸ¥çµæœ
for platform, result in results.items():
    print(f"{platform}: {result.get('success')}")
```

---

## é«˜ç´šç”¨æ³•

### 1. é †åºå·¥ä½œæµç·¨æ’

```python
from ai_automation_framework.integrations.workflow_automation_unified import (
    WorkflowOrchestrator
)

orchestrator = WorkflowOrchestrator(manager)

# å®šç¾©é †åºæ­¥é©Ÿ
steps = [
    {
        "platform": WorkflowPlatform.N8N,
        "workflow_id": "extract_data",
        "data": {"source": "database"}
    },
    {
        "platform": WorkflowPlatform.MAKE,
        "workflow_id": "transform_data",
        "use_previous_output": True  # ä½¿ç”¨å‰ä¸€æ­¥çš„è¼¸å‡º
    },
    {
        "platform": WorkflowPlatform.ZAPIER,
        "workflow_id": "notify_users",
        "use_previous_output": True
    }
]

result = orchestrator.execute_sequential(steps)
```

### 2. ä¸¦è¡Œå·¥ä½œæµåŸ·è¡Œ

```python
# å®šç¾©ä¸¦è¡Œä»»å‹™
workflows = [
    {
        "platform": WorkflowPlatform.N8N,
        "workflow_id": "send_email",
        "data": {"to": "admin@example.com"}
    },
    {
        "platform": WorkflowPlatform.ZAPIER,
        "workflow_id": "log_event",
        "data": {"event_type": "notification_sent"}
    },
    {
        "platform": WorkflowPlatform.MAKE,
        "workflow_id": "update_crm",
        "data": {"customer_id": "CUST-123"}
    }
]

# ä¸¦è¡ŒåŸ·è¡Œ
result = orchestrator.execute_parallel(workflows)
```

### 3. éŒ¯èª¤è™•ç†å’Œé‡è©¦

```python
import time

def trigger_with_retry(manager, platform, workflow_id, data, max_retries=3):
    """å¸¶é‡è©¦çš„å·¥ä½œæµè§¸ç™¼"""
    for attempt in range(max_retries):
        result = manager.trigger_workflow(platform, workflow_id, data)

        if result.get('success'):
            return result

        print(f"å˜—è©¦ {attempt + 1} å¤±æ•—ï¼Œé‡è©¦ä¸­...")
        if attempt < max_retries - 1:
            time.sleep(2 ** attempt)  # æŒ‡æ•¸é€€é¿

    return result

# ä½¿ç”¨
result = trigger_with_retry(
    manager,
    WorkflowPlatform.N8N,
    "critical_workflow",
    {"data": "important"}
)
```

### 4. æ¢ä»¶åŸ·è¡Œ

```python
def conditional_workflow(manager, condition, workflow_a_id, workflow_b_id):
    """æ ¹æ“šæ¢ä»¶åŸ·è¡Œä¸åŒçš„å·¥ä½œæµ"""

    if condition:
        workflow_id = workflow_a_id
        platform = WorkflowPlatform.N8N
    else:
        workflow_id = workflow_b_id
        platform = WorkflowPlatform.ZAPIER

    return manager.trigger_workflow(
        platform=platform,
        workflow_id=workflow_id,
        data={"condition": condition}
    )

# ä½¿ç”¨
is_premium_customer = True
result = conditional_workflow(
    manager,
    is_premium_customer,
    "premium_workflow",
    "standard_workflow"
)
```

---

## æœ€ä½³å¯¦è¸

### 1. ç’°å¢ƒè®Šé‡ç®¡ç†

```python
import os
from dotenv import load_dotenv

# åŠ è¼‰ç’°å¢ƒè®Šé‡
load_dotenv()

manager = UnifiedWorkflowManager()

# ä½¿ç”¨ç’°å¢ƒè®Šé‡
manager.register_n8n(
    base_url=os.getenv("N8N_BASE_URL"),
    api_key=os.getenv("N8N_API_KEY")
)
```

### 2. æ—¥èªŒè¨˜éŒ„

```python
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def trigger_with_logging(manager, platform, workflow_id, data):
    """å¸¶æ—¥èªŒçš„å·¥ä½œæµè§¸ç™¼"""
    logger.info(f"è§¸ç™¼å·¥ä½œæµ: {platform.value}/{workflow_id}")

    result = manager.trigger_workflow(platform, workflow_id, data)

    if result.get('success'):
        logger.info(f"åŸ·è¡ŒæˆåŠŸ: {result.get('data', {}).get('id')}")
    else:
        logger.error(f"åŸ·è¡Œå¤±æ•—: {result.get('error')}")

    return result
```

### 3. é…ç½®ç®¡ç†

```python
from dataclasses import dataclass
from typing import Dict, Any

@dataclass
class WorkflowConfig:
    platform: WorkflowPlatform
    workflow_id: str
    default_data: Dict[str, Any]
    retry_count: int = 3
    timeout: int = 300

# å®šç¾©é…ç½®
WORKFLOWS = {
    "order_notification": WorkflowConfig(
        platform=WorkflowPlatform.N8N,
        workflow_id="notify_order",
        default_data={"channel": "email"},
        retry_count=3
    ),
    "data_sync": WorkflowConfig(
        platform=WorkflowPlatform.AIRFLOW,
        workflow_id="sync_dag",
        default_data={},
        timeout=600
    )
}

# ä½¿ç”¨é…ç½®
config = WORKFLOWS["order_notification"]
result = manager.trigger_workflow(
    platform=config.platform,
    workflow_id=config.workflow_id,
    data={**config.default_data, "order_id": "ORD-123"}
)
```

### 4. ç›£æ§å’Œå‘Šè­¦

```python
def monitor_workflow_execution(manager, platform, execution_id, alert_func=None):
    """ç›£æ§å·¥ä½œæµåŸ·è¡Œä¸¦åœ¨å¤±æ•—æ™‚å‘Šè­¦"""
    result = manager.get_workflow_status(platform, execution_id)

    if not result.get('success'):
        error_msg = f"å·¥ä½œæµåŸ·è¡Œå¤±æ•—: {result.get('error')}"

        if alert_func:
            alert_func(error_msg)
        else:
            print(f"âŒ {error_msg}")

        return False

    return True

# ä½¿ç”¨
def send_alert(message):
    # ç™¼é€å‘Šè­¦åˆ° Slack/Email ç­‰
    print(f"ğŸš¨ å‘Šè­¦: {message}")

monitor_workflow_execution(
    manager,
    WorkflowPlatform.N8N,
    "exec_123",
    alert_func=send_alert
)
```

---

## æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ

#### 1. é€£æ¥è¶…æ™‚

```python
# å¢åŠ è¶…æ™‚æ™‚é–“
from ai_automation_framework.integrations.n8n_integration_enhanced import N8NEnhanced

n8n = N8NEnhanced(
    base_url="http://localhost:5678",
    api_key="key",
    timeout=60  # 60 ç§’è¶…æ™‚
)
```

#### 2. API å¯†é‘°ç„¡æ•ˆ

```bash
# é©—è­‰ç’°å¢ƒè®Šé‡
echo $N8N_API_KEY
echo $ZAPIER_API_KEY
echo $MAKE_API_TOKEN

# é‡æ–°åŠ è¼‰ç’°å¢ƒè®Šé‡
source .env  # Bash
# æˆ–
dotenv load  # Python-dotenv
```

#### 3. Webhook URL éŒ¯èª¤

```python
# é©—è­‰ Webhook URL
webhook_url = "https://hooks.zapier.com/hooks/catch/xxx/yyy/"

# æ¸¬è©¦é€£æ¥
import requests
response = requests.post(webhook_url, json={"test": "data"})
print(response.status_code)  # æ‡‰è©²è¿”å› 200
```

#### 4. æ•¸æ“šæ ¼å¼éŒ¯èª¤

```python
# ç¢ºä¿æ•¸æ“šæ˜¯ JSON å¯åºåˆ—åŒ–çš„
import json

data = {
    "string": "text",
    "number": 123,
    "boolean": True,
    "list": [1, 2, 3],
    "dict": {"key": "value"}
}

# é©—è­‰
try:
    json.dumps(data)
    print("âœ… æ•¸æ“šæ ¼å¼æ­£ç¢º")
except TypeError as e:
    print(f"âŒ æ•¸æ“šæ ¼å¼éŒ¯èª¤: {e}")
```

### èª¿è©¦æŠ€å·§

```python
# å•Ÿç”¨è©³ç´°æ—¥èªŒ
import logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# æ‰“å°è«‹æ±‚å’ŒéŸ¿æ‡‰
result = manager.trigger_workflow(
    platform=WorkflowPlatform.N8N,
    workflow_id="workflow_id",
    data={"debug": True}
)

print(json.dumps(result, indent=2))
```

---

## ç¸½çµ

é€šéæœ¬æŒ‡å—ï¼Œæ‚¨å·²ç¶“å­¸æœƒäº†ï¼š

- âœ… é›†æˆå¤šå€‹å·¥ä½œæµè‡ªå‹•åŒ–å¹³å°
- âœ… ä½¿ç”¨çµ±ä¸€æ¥å£ç®¡ç†å·¥ä½œæµ
- âœ… å¯¦ç¾é †åºå’Œä¸¦è¡Œå·¥ä½œæµç·¨æ’
- âœ… è™•ç†éŒ¯èª¤å’Œå¯¦ç¾é‡è©¦é‚è¼¯
- âœ… æ‡‰ç”¨æœ€ä½³å¯¦è¸å’Œç›£æ§ç­–ç•¥

## æ›´å¤šè³‡æº

- [n8n æ–‡æª”](https://docs.n8n.io/)
- [Make æ–‡æª”](https://www.make.com/en/help)
- [Zapier æ–‡æª”](https://platform.zapier.com/docs/)
- [Airflow æ–‡æª”](https://airflow.apache.org/docs/)

---

**æœ€å¾Œæ›´æ–°**: 2025-01-XX
**ç‰ˆæœ¬**: 2.0.0
